{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import tensorflow as tf\n",
    "import models\n",
    "import metrics\n",
    "import losses\n",
    "import dataloaders as dl\n",
    "from utils import ideatlas, deeplearning\n",
    "from tqdm.keras import TqdmCallback\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available GPUs\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    try:\n",
    "        for gpu in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPUs detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "config = load_config('./config.json')\n",
    "\n",
    "batch = config[\"batch\"]\n",
    "epochs = config[\"epochs\"]\n",
    "data_dir = config['data_dir']\n",
    "chk_dir = config['chkdir']\n",
    "log_dir = config[\"logdir\"]\n",
    "city = data_dir.split('/')[2]\n",
    "print(city)\n",
    "inputs = config[\"dataset\"]\n",
    "inputs_str = \"_\".join(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes = {input: config[\"in_shape\"][input] for input in inputs}\n",
    "for input, shape in input_shapes.items():\n",
    "    # print(f\"Shape of {input} is {shape}\")\n",
    "    dim = config[\"in_shape\"][input]\n",
    "    X,Y = dim[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training model on', city.capitalize(), 'dataset')\n",
    "print('Input: ', inputs)\n",
    "# train_s1 = ideatlas.load_image(data_dir + 'train', X, Y)\n",
    "train_s2 = ideatlas.load_image(data_dir + 'train', X, Y)\n",
    "train_bd = ideatlas.load_bd(data_dir + 'train', X, Y)\n",
    "train_label = ideatlas.load_mask(data_dir + 'train', X, Y, config['n_classes'])\n",
    "\n",
    "#Validation\n",
    "# valid_s1 = ideatlas.load_image(data_dir + 'val', X, Y)\n",
    "valid_s2 = ideatlas.load_image(data_dir + 'val/', X, Y)\n",
    "valid_bd = ideatlas.load_bd(data_dir + 'val', X, Y)\n",
    "valid_label = ideatlas.load_mask(data_dir + 'val/', X, Y, config['n_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training data {train_label.shape, train_s2.shape, train_bd.shape}') \n",
    "print(f'Validation data {valid_label.shape, valid_s2.shape, valid_bd.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s2 = ideatlas.norm_s2(train_s2)\n",
    "valid_s2 = ideatlas.norm_s2(valid_s2)\n",
    "\n",
    "# train_images  = [train_s2, train_bd]\n",
    "# valid_images  = [valid_s2, valid_bd]\n",
    "\n",
    "train_images = np.concatenate((train_s2, train_bd), axis=-1)\n",
    "valid_images = np.concatenate((valid_s2, valid_bd), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min and Max value in train image: \", train_s2.min(), train_s2.max())\n",
    "print(\"Labels in the mask are : \", np.unique(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainData = glob(os.path.join(config['vhr'] + 'train_im', 'HR*.tif'))\n",
    "# validData = glob(os.path.join(config['vhr'] + 'valid_im', 'HR*.tif'))\n",
    "\n",
    "# train_datagen = ideatlas.image_generator(trainData, batch_size, (X,Y,4), config['n_classes'], config['vhr'] + 'train_gt')\n",
    "# validation_datagen = ideatlas.image_generator(validData, batch_size, (X,Y,4), config['n_classes'], config['vhr'] + 'valid_gt')\n",
    "\n",
    "# ideatlas.patch_class_proportion(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Randomly visualizing the training images and its corresponding mask\n",
    "# ideatlas.plot_samples_datagen(train_datagen)\n",
    "ideatlas.plot_samples_1hot(train_s2, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = ideatlas.calculate_class_weights(train_label)\n",
    "print(f'Class weight: {class_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "\n",
    "import models.fcn\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=class_weights) \n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "t_loss =  dice_loss + (2 * focal_loss)\n",
    "j_loss = sm.losses.JaccardLoss(class_weights=class_weights, class_indexes=None, per_image=False, smooth=1e-05)\n",
    "# model = models.mbcnn(CL=config[\"n_classes\"], input_shapes=input_shapes, dropout_rate=0.2, batch_norm=True, drop_train=False)\n",
    "# model = segmentation.lightunet(input_shape = (X,Y,4), NUM_CLASSES=3, dropout_rate=0.2, batch_norm=True)\n",
    "# model = segmentation.mbcnn(CL=config[\"n_classes\"], input_shapes=input_shapes, dropout_rate=0.2, batch_norm=True, drop_train=False)\n",
    "model = models.fcndk6(input_shapes=(X,Y,11), CL=config[\"n_classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"lr\"]),\n",
    "loss=t_loss,\n",
    "# loss = losses.FocalLoss(),\n",
    "metrics=[sm.metrics.FScore(threshold=0.5, class_indexes=None, per_image=False, smooth=1e-05, name='f1')]\n",
    ")\n",
    "print(f'| Model -> {model.name} | Parameters -> {model.count_params()} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss={\n",
    "#         'regression': 'mean_squared_error',\n",
    "#         'segmentation': losses.FocalLoss()\n",
    "#     },\n",
    "#     loss_weights={\n",
    "#         'regression': 1.0,\n",
    "#         'segmentation': 1.0\n",
    "#     },\n",
    "#     metrics={\n",
    "#         'regression': ['mean_squared_error'],\n",
    "#         'segmentation': [metrics.IoU()]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(chk_dir):\n",
    "    if i.endswith('.h5'):\n",
    "        os.remove(os.path.join(chk_dir, i))\n",
    "\n",
    "# Delete .csv files in ./log/\n",
    "for j in os.listdir(log_dir):\n",
    "    if j.endswith('.csv'):\n",
    "        os.remove(os.path.join(log_dir, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for saving logs, model weights, early stopping and learning rate schedule\n",
    "# steps_per_epoch=len(trainData) // batch_size\n",
    "steps_per_epoch = len(train_images[0])\n",
    "callbacks = [\n",
    "    # deeplearning.LRWarmup(\n",
    "    #     warmup_steps=steps_per_epoch,\n",
    "    #     target=config['lr'],\n",
    "    #     verbose=0,\n",
    "    # ),\n",
    "    # tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    #     monitor=\"val_loss\",\n",
    "    #     mode=\"min\",\n",
    "    #     factor=0.1,\n",
    "    #     patience=10,\n",
    "    #     verbose=0,\n",
    "    # ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=15,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(f'./{chk_dir}/{city}_{inputs_str}_{model.name}.weights.h5'),\n",
    "        monitor=f\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        os.path.join(\"log\", f\"{city}_{inputs_str}_{model.name}_log.csv\"),\n",
    "    ),\n",
    "    deeplearning.training_progress(epochs, steps_per_epoch)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_label,\n",
    "        batch_size=batch,\n",
    "        steps_per_epoch = math.ceil(len(train_s2) / batch),\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(valid_images, valid_label),\n",
    "        validation_steps = len(valid_s2) // batch,\n",
    "        verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "# Fit with multi task learning\n",
    "# hist = model.fit(train_images, [train_bd, train_masks],\n",
    "#                 batch_size = batch_size,\n",
    "#                 steps_per_epoch = len(train_s2) // batch_size,\n",
    "#                 epochs=epochs,\n",
    "#                 callbacks=callbacks, \n",
    "#                 validation_data=(valid_images, [valid_bd, valid_masks]),\n",
    "#                 validation_steps = len(valid_s2) // batch_size)\n",
    "\n",
    "#Fit with dataloader\n",
    "# hist = model.fit(train_datagen,\n",
    "#                     steps_per_epoch=len(trainData) // batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     callbacks=[callbacks],\n",
    "#                     validation_data=validation_datagen,\n",
    "#                     validation_steps=len(validData) // batch_size,\n",
    "#                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # ## Plot training log\n",
    "import pandas as pd\n",
    "history = pd.read_csv(os.path.join(\"log\", f\"{city}_{inputs_str}_{model.name}_log.csv\"))\n",
    "ideatlas.plot_log(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print('Testing a model')\n",
    "model = models.mbcnn(CL=config[\"n_classes\"], input_shapes=input_shapes, dropout_rate=0.25, batch_norm=True, drop_train=False)\n",
    "# model = segmentation.lightunet(input_shape = (X,Y,11), NUM_CLASSES=3, dropout_rate=0.2, batch_norm=True)\n",
    "# weight =(f'./{chk_dir}/{city}_{inputs_str}_{model.name}_weight.h5')\n",
    "weight = '/data/experiments-tf/checkpoint/mbcnn_weights/buenos_aires_s2_morph_mbcnn_weight_f58.h5'\n",
    "print(weight)\n",
    "model.load_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "# test_s1 = ideatlas.load_image(config['data_dir'] + 'image/test_s1', X, Y)\n",
    "test_s2 = ideatlas.load_image(config['data_dir'] + 'test', X, Y)\n",
    "test_bd = ideatlas.load_bd(config['data_dir'] + 'test', X, Y)\n",
    "test_label = ideatlas.load_mask(config['data_dir'] + 'test', X, Y, config['n_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_s2 = ideatlas.norm_s2(test_s2)\n",
    "# test_images = [test_s2, test_bd]\n",
    "test_images = np.concatenate((test_s2, test_bd), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test data {test_label.shape,  test_s2.shape,  test_bd.shape}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class_report, cm = metrics.class_report(test_images, test_label, model)\n",
    "# class_report, cm = metrics.class_report_mtcnn(test_images, test_masks, model)\n",
    "# class_report, cm = metrics.class_report_large_patches(test_images, test_label, model, 128, 128)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the confusion matrix\n",
    "ideatlas.plot_confusion_matrix(cm, save_plot=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_prediction_from_dataloader(test_images, test_label, model)\n",
    "# ideatlas.plot_prediction(test_images, test_label, model)\n",
    "# ideatlas.plot_prediction_mbcnn(test_images, test_label, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "colors = ['#636363', '#ced2d2', '#fd0006'] #non-builtup, formal, informal\n",
    "customCmap = mcolors.ListedColormap(colors)\n",
    "def plot_prediction(test_images, test_label, model):\n",
    "    test_img_number = random.randint(0, len(test_images)-1)\n",
    "    print(f'Test image number: {test_img_number}')\n",
    "    test_img = test_images[test_img_number]\n",
    "    ground_truth=test_label[test_img_number]\n",
    "    test_img_input=np.expand_dims(test_img, 0)\n",
    "    test_pred = model.predict(test_img_input) \n",
    "    test_prediction = np.argmax(test_pred, axis=3)[0,:,:] \n",
    "\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.subplot(231)\n",
    "    plt.title('Test image', fontsize = 25)\n",
    "    plt.imshow(test_img[..., [2,1,0]]*3)\n",
    "    plt.subplot(232)\n",
    "    plt.title('Reference', fontsize = 25)\n",
    "    plt.imshow(ground_truth[:,:,:], cmap=customCmap, alpha=0.75, vmin=0, vmax=2)\n",
    "    plt.subplot(233)\n",
    "    plt.title('Prediction', fontsize = 25)\n",
    "    plt.imshow(test_prediction, cmap=customCmap, alpha=0.75, vmin=0, vmax=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_prediction(test_images, test_label, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_density, predicted_masks = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mtcnn_inference(N_CLASSES, raster_paths, ndbi_path, model, prediction_path, vector_path):\n",
    "#     src = rio.open(raster_paths)\n",
    "#     ndbi_src = rio.open(ndbi_path)\n",
    "\n",
    "#     ds = src.read()\n",
    "#     ndbi_ds = ndbi_src.read(1)  # Assuming NDBI is a single band raster\n",
    "\n",
    "#     large_image = np.moveaxis(ds, 0, -1)\n",
    "#     large_ndbi = ndbi_ds\n",
    "\n",
    "#     patch_height, patch_width = model.inputs[0].shape[1], model.inputs[0].shape[2]\n",
    "#     stride = int(patch_height / 4)  # Set the desired overlap between patches\n",
    "#     image_height, image_width = large_image.shape[:2]\n",
    "#     y_pred = np.zeros((image_height, image_width, N_CLASSES))\n",
    "#     count_map = np.zeros((image_height, image_width, N_CLASSES))\n",
    "\n",
    "#     total_iterations = ((image_height - patch_height + 1) // stride) * ((image_width - patch_width + 1) // stride)\n",
    "#     pbar = tqdm(total=total_iterations, desc=\"Running Full Inference:\")\n",
    "\n",
    "#     for y in range(0, image_height - patch_height + 1, stride):\n",
    "#         for x in range(0, image_width - patch_width + 1, stride):\n",
    "#             patch = large_image[y:y + patch_height, x:x + patch_width]\n",
    "#             ndbi_patch = large_ndbi[y:y + patch_height, x:x + patch_width]\n",
    "\n",
    "#             # Expand dimensions for model input\n",
    "#             input_patch = np.expand_dims(patch, axis=0)\n",
    "#             input_ndbi_patch = np.expand_dims(ndbi_patch, axis=(0, -1))\n",
    "\n",
    "#             # Predict using the model\n",
    "#             patch_predictions = model.predict([input_patch, input_ndbi_patch], verbose=0)\n",
    "\n",
    "#             # Assume patch_predictions[1] is for segmentation task\n",
    "#             y_pred[y:y + patch_height, x:x + patch_width] += patch_predictions[1][0]\n",
    "#             count_map[y:y + patch_height, x:x + patch_width] += 1\n",
    "\n",
    "#             pbar.update(1)\n",
    "    \n",
    "#     pbar.close()\n",
    "#     averaged_predictions = y_pred / count_map\n",
    "#     final_pred = np.argmax(averaged_predictions, axis=-1)\n",
    "\n",
    "#     # Clip the predicted raster using the vector boundary\n",
    "#     def clip_raster_with_vector(raster_array, profile, vector_path, output_path):\n",
    "#         with rio.Env():\n",
    "#             with rio.open(output_path, 'w', **profile) as dst:\n",
    "#                 dst.write(raster_array.astype(rio.int8), 1)\n",
    "#             clipper = gpd.read_file(vector_path)\n",
    "#             with rio.open(output_path) as src:\n",
    "#                 clipped, clipped_transform = mask(src, clipper.geometry, crop=True)\n",
    "#                 profile.update({\n",
    "#                     'height': clipped.shape[1],\n",
    "#                     'width': clipped.shape[2],\n",
    "#                     'transform': clipped_transform\n",
    "#                 })\n",
    "#                 with rio.open(output_path, 'w', **profile) as dst:\n",
    "#                     dst.write(clipped)\n",
    "\n",
    "#     # Save the predicted array as a georeferenced raster and then clip\n",
    "#     with rio.Env():\n",
    "#         profile = src.profile\n",
    "#         profile.update(\n",
    "#             dtype=rio.int8,\n",
    "#             count=1,\n",
    "#             width=final_pred.shape[-1],\n",
    "#             height=final_pred.shape[-2],\n",
    "#             transform=src.transform,\n",
    "#             compress='lzw'\n",
    "#         )\n",
    "#         temp_pred_path = 'temp_prediction.tif'\n",
    "#         with rio.open(temp_pred_path, 'w', **profile) as dst:\n",
    "#             dst.write(final_pred.astype(rio.int8), 1)\n",
    "\n",
    "#         # Clip the temporary prediction raster using the city boundary\n",
    "#         clip_raster_with_vector(final_pred, profile, vector_path, prediction_path)\n",
    "\n",
    "#     print(f\"Prediction map saved and clipped successfully to {prediction_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rasterio as rio\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from rasterio.mask import mask\n",
    "# import geopandas as gpd\n",
    "\n",
    "# mtcnn_inference(\n",
    "#     N_CLASSES=3, \n",
    "#     raster_paths=(f'./dataset/{city}/original/2019/S2_2019_SR.tif'), \n",
    "#     ndbi_path=(f'./dataset/{city}/original/Density.tif'), \n",
    "#     model=model, \n",
    "#     prediction_path=f'./prediction/{city}/{city}_{inputs_str}_{model.name}.tif',\n",
    "#     vector_path='/data/training_data/city_boundary/nairobi_admin_boundary.geojson'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
